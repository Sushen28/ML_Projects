{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN94MlvcGMxI8beWWfOY2Ta",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sushen28/ML_Projects/blob/main/FE_text_summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this program summarizes the texts that are present in files. Upload a file in 'test.txt' name."
      ],
      "metadata": {
        "id": "c6tSEdgWQ_H4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def read_article(file_name):\n",
        "    file = open(file_name, \"r\")\n",
        "    filedata = file.readlines()\n",
        "    article = filedata[0].split(\". \")\n",
        "    sentences = []\n",
        "\n",
        "    for sentence in article:\n",
        "        print(sentence)\n",
        "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
        "    sentences.pop()\n",
        "\n",
        "    return sentences\n",
        "\n",
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        "\n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        "\n",
        "    all_words = list(set(sent1 + sent2))\n",
        "\n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        "\n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        "\n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        "\n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        "\n",
        "def build_similarity_matrix(sentences, stop_words):\n",
        "    # Create an empty similarity matrix\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "\n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2: #ignore if both are same sentences\n",
        "                continue\n",
        "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "\n",
        "def generate_summary(file_name, top_n=5):\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "\n",
        "    # Step 1 - Read text anc split it\n",
        "    sentences =  read_article(file_name)\n",
        "\n",
        "    # Step 2 - Generate Similary Martix across sentences\n",
        "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    # Step 3 - Rank sentences in similarity martix\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    # Step 4 - Sort the rank and pick top sentences\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)\n",
        "\n",
        "    for i in range(top_n):\n",
        "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
        "\n",
        "    # Step 5 - Offcourse, output the summarize texr\n",
        "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
        "\n",
        "# let's begin\n",
        "generate_summary( \"test.txt\", 2)"
      ],
      "metadata": {
        "id": "1V-1vgON0uEi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def read_article(file_name):\n",
        "    file = open(file_name, \"r\")\n",
        "    filedata = file.readlines()\n",
        "    article = filedata[0].split(\". \")\n",
        "    sentences = []\n",
        "\n",
        "    for sentence in article:\n",
        "        print(sentence)\n",
        "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
        "    sentences.pop()\n",
        "\n",
        "    return sentences\n",
        "\n",
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        "\n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        "\n",
        "    all_words = list(set(sent1 + sent2))\n",
        "\n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        "\n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        "\n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        "\n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        "\n",
        "def build_similarity_matrix(sentences, stop_words):\n",
        "    # Create an empty similarity matrix\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "\n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2: #ignore if both are same sentences\n",
        "                continue\n",
        "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "\n",
        "def generate_summary(file_name, top_n=5):\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "\n",
        "    # Step 1 - Read text anc split it\n",
        "    sentences =  read_article(file_name)\n",
        "\n",
        "    # Step 2 - Generate Similary Martix across sentences\n",
        "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    # Step 3 - Rank sentences in similarity martix\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    # Step 4 - Sort the rank and pick top sentences\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)\n",
        "\n",
        "    for i in range(top_n):\n",
        "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
        "\n",
        "    # Step 5 - Offcourse, output the summarize texr\n",
        "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
        "\n",
        "# let's begin\n",
        "generate_summary( \"Asiacup.txt\", 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5APJpZwpViM",
        "outputId": "c9e6c5a6-6290-490a-9067-13381e95c4b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sri Lanka lifted the Asia Cup after defeating Pakistan in the finale in the Dubai International Stadium on Sunday\n",
            "Bhanuka Rajapaksa’s flamboyant 70-run knock helped the Lankans put up a challenging total of 170 runs in the first innings\n",
            "He enabled the Dasun Shanaka-led side to recuperate from a position of bother as they were down at 57/5 at one point in time.\n",
            "\n",
            "Indexes of top ranked_sentence order are  [(0.5, ['Sri', 'Lanka', 'lifted', 'the', 'Asia', 'Cup', 'after', 'defeating', 'Pakistan', 'in', 'the', 'finale', 'in', 'the', 'Dubai', 'International', 'Stadium', 'on', 'Sunday']), (0.5, ['Bhanuka', 'Rajapaksa’s', 'flamboyant', '70-run', 'knock', 'helped', 'the', 'Lankans', 'put', 'up', 'a', 'challenging', 'total', 'of', '170', 'runs', 'in', 'the', 'first', 'innings'])]\n",
            "Summarize Text: \n",
            " Sri Lanka lifted the Asia Cup after defeating Pakistan in the finale in the Dubai International Stadium on Sunday. Bhanuka Rajapaksa’s flamboyant 70-run knock helped the Lankans put up a challenging total of 170 runs in the first innings\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def read_article(file_name):\n",
        "    file = open(file_name, \"r\")\n",
        "    filedata = file.readlines()\n",
        "    article = filedata[0].split(\". \")\n",
        "    sentences = []\n",
        "\n",
        "    for sentence in article:\n",
        "        print(sentence)\n",
        "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
        "    sentences.pop()\n",
        "\n",
        "    return sentences\n",
        "\n",
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        "\n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        "\n",
        "    all_words = list(set(sent1 + sent2))\n",
        "\n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        "\n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        "\n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        "\n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        "\n",
        "def build_similarity_matrix(sentences, stop_words):\n",
        "    # Create an empty similarity matrix\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "\n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2: #ignore if both are same sentences\n",
        "                continue\n",
        "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "\n",
        "def generate_summary(file_name, top_n=5):\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "\n",
        "    # Step 1 - Read text anc split it\n",
        "    sentences =  read_article(file_name)\n",
        "\n",
        "    # Step 2 - Generate Similary Martix across sentences\n",
        "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    # Step 3 - Rank sentences in similarity martix\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    # Step 4 - Sort the rank and pick top sentences\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)\n",
        "\n",
        "    for i in range(top_n):\n",
        "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
        "\n",
        "    # Step 5 - Offcourse, output the summarize texr\n",
        "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
        "\n",
        "# let's begin\n",
        "generate_summary( \"Cricket.txt\", 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cIs_1KEpVf3",
        "outputId": "70cf1298-c892-4dac-a414-985519ae5062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When predictions for Asia Cup 2022 were made, nobody thought that Sri Lanka would emerge as champions\n",
            "India were most people’s clear favourites, followed by Pakistan\n",
            "But a gritty and determined young Sri Lankan team proved everyone wrong.\n",
            "\n",
            "Indexes of top ranked_sentence order are  [(0.5, ['When', 'predictions', 'for', 'Asia', 'Cup', '2022', 'were', 'made,', 'nobody', 'thought', 'that', 'Sri', 'Lanka', 'would', 'emerge', 'as', 'champions']), (0.5, ['India', 'were', 'most', 'people’s', 'clear', 'favourites,', 'followed', 'by', 'Pakistan'])]\n",
            "Summarize Text: \n",
            " When predictions for Asia Cup 2022 were made, nobody thought that Sri Lanka would emerge as champions. India were most people’s clear favourites, followed by Pakistan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def read_article(file_name):\n",
        "    file = open(file_name, \"r\")\n",
        "    filedata = file.readlines()\n",
        "    article = filedata[0].split(\". \")\n",
        "    sentences = []\n",
        "\n",
        "    for sentence in article:\n",
        "        print(sentence)\n",
        "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
        "    sentences.pop()\n",
        "\n",
        "    return sentences\n",
        "\n",
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        "\n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        "\n",
        "    all_words = list(set(sent1 + sent2))\n",
        "\n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        "\n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        "\n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        "\n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        "\n",
        "def build_similarity_matrix(sentences, stop_words):\n",
        "    # Create an empty similarity matrix\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "\n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2: #ignore if both are same sentences\n",
        "                continue\n",
        "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "\n",
        "def generate_summary(file_name, top_n=5):\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "\n",
        "    # Step 1 - Read text anc split it\n",
        "    sentences =  read_article(file_name)\n",
        "\n",
        "    # Step 2 - Generate Similary Martix across sentences\n",
        "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    # Step 3 - Rank sentences in similarity martix\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    # Step 4 - Sort the rank and pick top sentences\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)\n",
        "\n",
        "    for i in range(top_n):\n",
        "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
        "\n",
        "    # Step 5 - Offcourse, output the summarize texr\n",
        "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
        "\n",
        "# let's begin\n",
        "generate_summary( \"gambler.txt\", 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yheIVhZzpVdP",
        "outputId": "311f0d4c-694a-4294-cabf-2538038e5836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Considering that I am working in Tesla and investing my money in stocks, then I should think like a gambler, I should know when to Hold, When to Fold, and When to walk away\n",
            "That’s all depends on prediction and probability residing with it\n",
            "Gambling is a part of human condition you can lose or win fortunes.\n",
            "\n",
            "Indexes of top ranked_sentence order are  [(0.5, ['That’s', 'all', 'depends', 'on', 'prediction', 'and', 'probability', 'residing', 'with', 'it']), (0.5, ['Considering', 'that', 'I', 'am', 'working', 'in', 'Tesla', 'and', 'investing', 'my', 'money', 'in', 'stocks,', 'then', 'I', 'should', 'think', 'like', 'a', 'gambler,', 'I', 'should', 'know', 'when', 'to', 'Hold,', 'When', 'to', 'Fold,', 'and', 'When', 'to', 'walk', 'away'])]\n",
            "Summarize Text: \n",
            " That’s all depends on prediction and probability residing with it. Considering that I am working in Tesla and investing my money in stocks, then I should think like a gambler, I should know when to Hold, When to Fold, and When to walk away\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def read_article(file_name):\n",
        "    file = open(file_name, \"r\")\n",
        "    filedata = file.readlines()\n",
        "    article = filedata[0].split(\". \")\n",
        "    sentences = []\n",
        "\n",
        "    for sentence in article:\n",
        "        print(sentence)\n",
        "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
        "    sentences.pop()\n",
        "\n",
        "    return sentences\n",
        "\n",
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        "\n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        "\n",
        "    all_words = list(set(sent1 + sent2))\n",
        "\n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        "\n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        "\n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        "\n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        "\n",
        "def build_similarity_matrix(sentences, stop_words):\n",
        "    # Create an empty similarity matrix\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "\n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2: #ignore if both are same sentences\n",
        "                continue\n",
        "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "\n",
        "def generate_summary(file_name, top_n=5):\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "\n",
        "    # Step 1 - Read text anc split it\n",
        "    sentences =  read_article(file_name)\n",
        "\n",
        "    # Step 2 - Generate Similary Martix across sentences\n",
        "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    # Step 3 - Rank sentences in similarity martix\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    # Step 4 - Sort the rank and pick top sentences\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)\n",
        "\n",
        "    for i in range(top_n):\n",
        "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
        "\n",
        "    # Step 5 - Offcourse, output the summarize texr\n",
        "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
        "\n",
        "# let's begin\n",
        "generate_summary( \"corona.txt\", 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbkJMDVjpVbC",
        "outputId": "5602ae3d-8091-4859-c44c-79b735b05c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is where it gets tricky and deeper than reps, reps, reps\n",
            "The idea that simple, sterile reps will build the database and vastly improve your skills works (fairly?) well for static examples\n",
            "The target is always the same and predictable but tracking a ball carrier is a massively variable action\n",
            "MASSIVELY variable\n",
            "The more variable a task, the less rote ‘memorization’ is the answer\n",
            "It becomes much more problem solving\n",
            "So if we are only practicing solving the problem on a very narrow bandwidth in training, what happens on gameday? Meaning, doing reps of the same thing over and over might not be the answer you need on gameday because you can’t copy and paste to all situations\n",
            "Only doing the simplest of drills may leave lots of blank spots in the database because the gameday version of the problem most likely looks nothing like what was practiced\n",
            "Let’s explore that\n",
            "\n",
            "Indexes of top ranked_sentence order are  [(0.18439402093706306, ['So', 'if', 'we', 'are', 'only', 'practicing', 'solving', 'the', 'problem', 'on', 'a', 'very', 'narrow', 'bandwidth', 'in', 'training,', 'what', 'happens', 'on', 'gameday?', 'Meaning,', 'doing', 'reps', 'of', 'the', 'same', 'thing', 'over', 'and', 'over', 'might', 'not', 'be', 'the', 'answer', 'you', 'need', 'on', 'gameday', 'because', 'you', 'can’t', 'copy', 'and', 'paste', 'to', 'all', 'situations']), (0.16171614930368822, ['MASSIVELY', 'variable']), (0.13060488571029255, ['The', 'target', 'is', 'always', 'the', 'same', 'and', 'predictable', 'but', 'tracking', 'a', 'ball', 'carrier', 'is', 'a', 'massively', 'variable', 'action']), (0.12077930861862657, ['The', 'more', 'variable', 'a', 'task,', 'the', 'less', 'rote', '‘memorization’', 'is', 'the', 'answer']), (0.11746142058048697, ['It', 'becomes', 'much', 'more', 'problem', 'solving']), (0.10697972831470456, ['Only', 'doing', 'the', 'simplest', 'of', 'drills', 'may', 'leave', 'lots', 'of', 'blank', 'spots', 'in', 'the', 'database', 'because', 'the', 'gameday', 'version', 'of', 'the', 'problem', 'most', 'likely', 'looks', 'nothing', 'like', 'what', 'was', 'practiced']), (0.08770204356545386, ['The', 'idea', 'that', 'simple,', 'sterile', 'reps', 'will', 'build', 'the', 'database', 'and', 'vastly', 'improve', 'your', 'skills', 'works', '(fairly?)', 'well', 'for', 'static', 'examples']), (0.07195753499422422, ['Here', 'is', 'where', 'it', 'gets', 'tricky', 'and', 'deeper', 'than', 'reps,', 'reps,', 'reps']), (0.018404907975460127, ['Let’s', 'explore', 'that'])]\n",
            "Summarize Text: \n",
            " So if we are only practicing solving the problem on a very narrow bandwidth in training, what happens on gameday? Meaning, doing reps of the same thing over and over might not be the answer you need on gameday because you can’t copy and paste to all situations. MASSIVELY variable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def read_article(file_name):\n",
        "    file = open(file_name, \"r\")\n",
        "    filedata = file.readlines()\n",
        "    article = filedata[0].split(\". \")\n",
        "    sentences = []\n",
        "\n",
        "    for sentence in article:\n",
        "        print(sentence)\n",
        "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
        "    sentences.pop()\n",
        "\n",
        "    return sentences\n",
        "\n",
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        "\n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        "\n",
        "    all_words = list(set(sent1 + sent2))\n",
        "\n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        "\n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        "\n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        "\n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        "\n",
        "def build_similarity_matrix(sentences, stop_words):\n",
        "    # Create an empty similarity matrix\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "\n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2: #ignore if both are same sentences\n",
        "                continue\n",
        "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "\n",
        "    return similarity_matrix\n",
        "with open('test.txt') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "#normalizing text\n",
        "\n",
        "texts = [[word.lower() for word in line.split()] for line in lines]\n",
        "\n",
        "# removing unicode errors\n",
        "import re\n",
        "lines = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", lines)\n",
        "\n",
        "#stemming\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "words_stem = stemmer.stem(lines)\n",
        "\n",
        "def generate_summary(file_name, top_n=5):\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "\n",
        "    # Step 1 - Read text anc split it\n",
        "    sentences =  read_article(file_name)\n",
        "\n",
        "    # Step 2 - Generate Similary Martix across sentences\n",
        "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    # Step 3 - Rank sentences in similarity martix\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    # Step 4 - Sort the rank and pick top sentences\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)\n",
        "\n",
        "    for i in range(top_n):\n",
        "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
        "\n",
        "    # Step 5 - Offcourse, output the summarize texr\n",
        "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
        "\n",
        "# let's begin\n",
        "generate_summary( \"test.txt\", 2)"
      ],
      "metadata": {
        "id": "wxP4xOYzpVYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWERS:"
      ],
      "metadata": {
        "id": "tvYGcY8BjEFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1)** **Two main strategies used in text summarization**\n",
        "\n",
        "The two main strategies used in text summarization are:\n",
        "\n",
        "Extraction : It means picking up the most important lines and phrases in the paragraph to create a summary\n",
        "\n",
        "Abstraction: It means shortening the long paragrahps in the paragraph and converting them to readable form."
      ],
      "metadata": {
        "id": "PsY6I_cGhyTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2)The feature used in text classification and the measure of it**\n",
        "\n",
        "we used feature engineering in text summarization to locate important sentences in text. Using feature engineering, we can look at the frequency of word distribution, It is measured by the similarities to the main documents."
      ],
      "metadata": {
        "id": "28mq9aRihyPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3) The similarity measure in the code:**\n",
        "\n",
        "The similarity measure used in the code is similarity matrix.It is measured by:\n",
        "greater the similarity of the the objects in the matrix, greater the value of measure."
      ],
      "metadata": {
        "id": "jDaxLEXfhyKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4) Using TF-IDF**\n",
        "\n",
        "Tf-idf could be used to find the meaning of the sentences in NLP. We can use it to generate a simplified summary for the given text paragraph.\n",
        "We can use it to compare how relevent are the summary and input file working. When I passed the code through tf-idf i got a similarity score of 2-3. it wouldnt how ever change the summary. Since the summary is the one that we are after,implementing tf-idf would not matter."
      ],
      "metadata": {
        "id": "7Naq08GshyFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5) The similarity between code with tf-idf with datasets:**\n",
        "\n",
        "The change in dataset is not affecting the tf-idf score, hence they are similar.\n",
        "\n",
        "**Analysis:**\n",
        "\n",
        "The TF-IDF scores to different datasets is same for all the 5 datsasets. So this means that the text summarization code is working perfectly for all datasets( although they contain different high frequency words)"
      ],
      "metadata": {
        "id": "_RqUhb3IbzsX"
      }
    }
  ]
}